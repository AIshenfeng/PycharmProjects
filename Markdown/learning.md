### 深度学习资料

https://blog.csdn.net/hjimce/article/details/45421595

### LRN——局部响应归一化

> https://blog.csdn.net/yangdashi888/article/details/77918311

> ai(x,y)表示在这个输出结构中的一个位置[a,b,c,d]，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第a张图的第d个通道下的高度为b宽度为c的点。
>
> 论文公式中的N表示通道数(channel)。a,n/2,k,α,β分别表示函数中的input,depth_radius,bias,alpha,beta，其中n/2,k,α,β都是自定义的，
>
> 特别注意一下∑叠加的方向是沿着通道方向的，即每个点值的平方和是沿着a中的第3维channel方向的，也就是一个点同方向的前面n/2个通道（最小为第0个通道）和后n/2个通道（最大为第d-1个通道）的点的平方和(共n+1个点)
>
>
>
>

​    动机： 侧抑制，被激活的神经元抑制相邻的神经元。
​    尤其在使用RELU的时候，“侧抑制很管用”
​    好处：增强泛化能力，做了平滑处理，对局部神经元创建竞争机制，是的响应大的值相对更大。

### BN

> https://blog.csdn.net/hjimce/article/details/50866313
>
> https://blog.csdn.net/guoyunfei20/article/details/78404577?locationNum=1&fps=1

> 在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。不过归一化层，可不像我们想象的那么简单，它是一个可学习、有参数的网络层。
>
> 网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们不得不降低学习率、小心地初始化。论文作者将分布发生变化称之为 *internal covariate shift*。大家应知道，一般在训练网络的时会将输入减去均值，还有些人甚至会对输入做白化等操作，目的是为了加快训练。为什么减均值、白化可以加快训练呢，这里做一个简单地说明：
>
> 首先，图像数据是高度相关的，假设其分布如下图a所示(简化为2维)。由于初始化的时候，我们的参数一般都是0均值的，因此开始的拟合y=Wx+b，基本过原点附近，如图b红色虚线。因此，网络需要经过多次学习才能逐步达到如紫色实线的拟合，即收敛的比较慢。如果我们对输入数据先作减均值操作，如图c，显然可以加快学习。更进一步的，我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练，如图d。 
>
> ![20171201152933175](/home/shenfeng/PycharmProjects/PycharmProjects/Markdown/20171201152933175.png)
>
>

### PCA数据降维

> https://blog.csdn.net/hjimce/article/details/45000221

> 主成分分析经常用于减少数据集的[维数](http://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0)，同时保持数据集中的对方差贡献最大的特征。PCA的数学定义是：一个[正交化](http://zh.wikipedia.org/wiki/%E6%AD%A3%E4%BA%A4%E5%8C%96)[线性变换](http://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2)，把数据变换到一个新的坐标系统中，使得这一数据的任何投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推
>
> PCA不仅仅是对高维数据进行降维，更重要的是经过降维去除了噪声，发现了数据中的模式。PCA把原先的n个特征用数目更少的m个特征取代，新特征是旧特征的线性组合，这些线性组合最大化样本方差，尽量使新的m个特征互不相关。从旧特征到新特征的映射捕获数据中的固有变异性。

### 白化whitening

> https://blog.csdn.net/hjimce/article/details/50864602?utm_source=blogxgwz1

> 白化的目的是去除输入数据的冗余信息。假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的；白化的目的就是降低输入的冗余性。
> ​    输入数据集X，经过白化处理后，新的数据X'满足两个性质：
> (1)特征之间相关性较低；
>
> (2)所有特征具有相同的方差。

### 先验概率、后验概率和共轭先验

> https://blog.csdn.net/baimafujinji/article/details/51374202

> 先验概率：P(X)先验概率分布，即关于某个变量 X 的概率分布，是在获得某些信息或者依据前，对 X 之不确定性所进行的猜测
>
> 似然函数：x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述**对于不同的模型参数，出现x这个样本点的概率是多少**P(x|θ) 
>
> > 最大似然估计 最大后验概率估计
>
> > https://blog.csdn.net/u011508640/article/details/72815981
>
> > 例如，如果一个硬币在10次抛落中正面均朝上，那硬币是均匀的（在抛落中，正反面的概率相等）概率是多少？这里用了概率这个词，但是实质上是“可能性”，也就是似然了
>
> 后验概率：P(θ|X)通常将先验概率乘以似然函数（Likelihood Function）再归一化后，得到后验概率分布。是在相关证据或者背景给定并纳入考虑之后的条件概率
>
> > 在给定的证据信息 X 下的概率，即 P(θ|X) 
>
> 我们用 P(θ) 表示概率分布函数，用 P(X|θ) 表示观测值 X 的似然函数。后验概率定义为 P(θ|X)=P(X|θ)P(θ)/P(X)，注意这也是贝叶斯定理所揭示的内容。
>
> 共轭先验：后验概率分布（正⽐于先验和似然函数的乘积）拥有与先验分布相同的函数形式。这个性质被叫做共轭性。共轭先验（conjugate prior）有着很重要的作⽤。它使得后验概率分布的函数形式与先验概率相同。
>
> >  例如，二项分布的参数之共轭先验就是我们前面介绍的 Beta 分布。多项式分布的参数之共轭先验则是 Dirichlet 分布，⽽⾼斯分布的均值之共轭先验是另⼀个⾼斯分布
>
> 对于任何指数族成员来说，都存在有一个共轭先验

### 指数分布族

> https://blog.csdn.net/saltriver/article/details/55105285?utm_source=blogxgwz2

> 指数分布族为很多重要而常用的概率分布提供了统一框架
>
> f(x|θ)=h(x)exp(η(θ)T(x)−A(θ))
>
> > θ是**自然参数(natural parameter)**，通常是一个实数； 
> > h(x)是**底层观测值（underlying measure）**； 
> > T(x)是**充分统计量（sufficient statistic）**； 
> > A(θ)被称为**对数规则化（log normalizer）**。 

### 似然函数取对数的原因

> https://blog.csdn.net/songyunli1111/article/details/81265011

> 联合概率是每个数据点概率的连乘：
>
> 两边取对数则可以将连乘化为连加
>
> 乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。
>
> 但其实可能更重要的一点是，因为概率值都在[0,1]之间，因此，概率的连乘将会变成一个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算